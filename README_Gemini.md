# twitter


1. [Требования к системе](#требования-к-системе)
   - [Функциональные требования](#функциональные-требования)
   - [Нефункциональные требования](#нефункциональные-требования)
2. [Оценка масштаба](#оценка-масштаба)
3. [Архитектура высокого уровня](#архитектура-высокого-уровня)
   - [Ключевые компоненты](#ключевые-компоненты)
   - [Дополнительные компоненты](#дополнительные-компоненты)
4. [Детальное проектирование](#детальное-проектирование)
5. [Детали реализации на Java](#детали-реализации-на-java)
## Требования к системе

### Функциональные требования

- **Публикация твитов**: Пользователи должны иметь возможность публиковать твиты длиной до 280 символов.
- **Лента новостей**: Пользователь должен видеть ленту новостей, состоящую из твитов людей, на которых он подписан (following).
- **Подписки**: Пользователь может подписываться (follow) и отписываться (unfollow) от других пользователей.
- **Лайки**: Пользователи могут ставить лайки твитам.
- **Профили пользователей**: У каждого пользователя есть свой профиль.
- Возможность лайков и ретвитов (опционально на базовом уровне).

### Нефункциональные требования

- **Высокая доступность**: Система должна быть доступна 24/7.
- **Масштабируемость**: Система должна справляться с огромным количеством пользователей и твитов (сотни миллионов пользователей, миллиарды твитов).
- **Низкая задержка**: Загрузка ленты новостей должна быть быстрой (несколько сотен миллисекунд).
- **Отказоустойчивость**: Система должна продолжать работать, даже если некоторые компоненты выйдут из строя.
- Eventual consistency допустима для лайков и репостов.

## Оценка масштаба

Давайте предположим следующие цифры, чтобы оценить нагрузку:
- **Количество пользователей**: 300 миллионов активных пользователей в месяц.
- **Количество твитов**: 100 миллионов твитов в день.
- **Чтение/Запись**: Предположим, соотношение чтения к записи составляет примерно 100:1. Это критически важная деталь, которая повлияет на архитектуру. Чтение будет гораздо более частой операцией, чем запись.

## Архитектура высокого уровня
Для такой масштабной системы нам понадобится распределенная архитектура, состоящая из нескольких микросервисов. Мы не можем использовать монолит, так как он не справится с нагрузкой.

### Ключевые компоненты:
1. **API Gateway**: Единая точка входа для всех запросов. Отвечает за маршрутизацию, аутентификацию и лимитирование.
2. **Микросервис пользователей (User Service)**: Управляет данными пользователей (профили, регистрации, аутентификация). Хранит данные в реляционной базе данных, например, PostgreSQL.
3. **Микросервис твитов (Tweet Service)**: Управляет публикацией, хранением и получением твитов.
4. **Микросервис ленты новостей (Timeline Service)**: Генерирует ленты новостей для пользователей. Это будет самый сложный и критически важный компонент.
5. **Микросервис подписок (Follower Service)**: Управляет связями между пользователями (кто кого читает).

### Дополнительные компоненты:
1. **Система кеширования (Caching System)**: Используем Redis для кеширования часто запрашиваемых данных (например, профилей пользователей, популярных твитов).
2. **Система очередей сообщений (Message Queue)**: Используем Apache Kafka для асинхронной обработки событий (например, публикации нового твита, отправки уведомлений).
3. **Хранилище больших данных (Big Data Storage)**: Используем Hadoop HDFS или S3 для хранения архивов твитов, которые не нужны в реальном времени.

## Детальное проектирование

### 1. Публикация твита (Запись) 

Когда пользователь отправляет твит, происходит следующее:
1. Запрос приходит на **API Gateway**, затем перенаправляется в **Tweet Service**.
2. **Tweet Service** сохраняет твит в базу данных. Для хранения твитов нам нужна очень быстрая база данных с возможностью горизонтального масштабирования. Отлично подойдет Cassandra или ScyllaDB, так как они хорошо справляются с большими объемами данных и высокой скоростью записи. Мы будем использовать denormalized схему, где твиты будут храниться по user_id.
3. **Tweet Service** отправляет сообщение в **Apache Kafka** (топик new-tweet-events). Это событие будет содержать ID твита и ID автора.

**Схема данных для Cassandra (пример):**
```
CREATE TABLE tweets_by_user (
    user_id UUID,
    tweet_id UUID,
    text VARCHAR,
    created_at TIMESTAMP,
    PRIMARY KEY (user_id, tweet_id)
) WITH CLUSTERING ORDER BY (tweet_id DESC);
```

### 2. Лента новостей (Чтение)
Это самая сложная часть, так как требуется высокая скорость. Есть два основных подхода: **Push Model** и **Pull Model**.
- **Push Model (Fan-out on Write)**: Когда пользователь публикует твит, этот твит сразу "пушится" (добавляется) в ленты новостей всех его подписчиков.
  - **Плюсы**: Чтение очень быстрое, так как лента уже сформирована.
  - **Минусы**: Требует огромных вычислительных ресурсов на запись, особенно для "суперзвезд" с миллионами подписчиков.
- **Pull Model (Fan-out on Read)**: Когда пользователь запрашивает ленту, система в реальном времени собирает твиты всех, на кого он подписан.
  - **Плюсы**: Запись очень простая.
  - **Минусы**: Чтение может быть очень медленным, так как нужно делать много запросов.

**Гибридный подход:**
1. **Для обычных пользователей**: Используем Push Model. Когда новый твит публикуется, Timeline Service получает событие из Kafka. Затем он запрашивает список подписчиков у Follower Service и добавляет этот твит в ленту каждого подписчика. Лента хранится в Redis как отсортированный список (sorted set) твитов, где значением является ID твита, а score — временная метка.
   - **Почему Redis?** Это in-memory база данных, которая идеально подходит для быстрых операций чтения и записи, и sorted set отлично подходит для хранения ленты в хронологическом порядке.
2. **Для "суперзвезд" (миллионы подписчиков):** Мы не будем "пушить" их твиты всем подписчикам. Вместо этого, подписчики будут использовать Pull Model. Когда такой подписчик запрашивает ленту, Timeline Service сначала запрашивает твиты "суперзвезд" напрямую из Tweet Service, затем добавляет их к уже сформированной ленте из Redis и возвращает объединенный результат.

**Схема данных для Redis (пример):**
```
Key: timeline:{user_id}
Value: sorted set of {tweet_id: timestamp}
```

### 3. Подписки и лайки
- **Подписки (Following)**: Храним данные в Follower Service. Это графовая структура (User A follows User B). Для этого идеально подойдет графовая база данных, например, Neo4j, но для простоты и масштабируемости можно использовать Cassandra или реляционную базу данных. Для Cassandra можно создать таблицу followers и following для быстрого доступа.

- **Схема данных для Cassandra (пример):**
```
CREATE TABLE followers_by_user (
    user_id UUID,
    follower_id UUID,
    PRIMARY KEY (user_id, follower_id)
);

CREATE TABLE following_by_user (
    user_id UUID,
    following_id UUID,
    PRIMARY KEY (user_id, following_id)
);
```

- **Лайки**: Лайки — это счетчики. Мы можем хранить их в Redis для быстрого доступа и инкрементировать счетчик с помощью INCR. Но для надежности и долгосрочного хранения, данные о том, кто именно поставил лайк, лучше хранить в постоянном хранилище, например, в Cassandra.

## Детали реализации на Java
- **Фреймворк: Spring Boot** — отличный выбор для быстрого создания микросервисов. Он поддерживает REST API, интеграцию с базами данных, Kafka, Redis и другими инструментами.
- **Асинхронность**: Используем CompletableFuture и Reactor/RxJava для неблокирующих операций, что позволит эффективно использовать ресурсы сервера.
- **Базы данных**:
  - **PostgreSQL** для User Service (для надежных транзакций).
  - **Cassandra**/ScyllaDB для Tweet Service и Follower Service (для высокой скорости записи и масштабирования).
  - **Redis** для Caching и Timeline Service (для низкой задержки).
- **Сообщения**: Kafka для асинхронного взаимодействия между сервисами.
- **Мониторинг и логирование**: Используем Prometheus/Grafana для метрик и ELK Stack (Elasticsearch, Logstash, Kibana) для агрегации логов.